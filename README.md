# ğŸ¬ Trend-Eval: Predicting Content Success Using Hybrid ML + LLMs

Trend-Eval is a hybrid machine learning pipeline that blends **traditional regression modeling** with **LLM-based sentiment and reasoning evaluation** to predict YouTube content performance (e.g., views per day).

## ğŸ§  Project Overview

This repository is designed to evaluate the impact of audio-visual and textual featuresâ€”extracted from trailers, metadata, and user commentsâ€”on content success. It also benchmarks LLM reasoning capabilities using evaluation suites like MMLU and HellaSwag.

## ğŸ” Key Features

- ğŸ“ˆ Predictive regression pipeline using `PyCaret` + `LightGBM`
- ğŸ§® Auto feature selection, normalization, and model tuning
- ğŸ§  LLM evaluation via Hugging Face on:
  - `hellaswag`, `mmlu`, `lambada`, `arc_challenge`, `winogrande`, `piqa`
- ğŸ“Š Visualization of model diagnostics (residuals, error, feature importance)
- ğŸ” Advanced charting (Seaborn, Matplotlib)
- ğŸ“‚ CSV export of predictions and metrics
- ğŸ” Integrated with **Zeno** for advanced LLM visualization and comparison

---


## ğŸ§ª Traditional ML Training Pipeline

[YouTube ID List]
      â†“
[Metadata + Comments API]
      â†“
[Text Analysis] â€”â€”â†’ [LLM sentiment / zero-shot / toxicity]
      â†“
[Video Downloader (pytube)]
      â†“
[Audio Track] â€”â€”â†’ [wav2vec2 + librosa + speechbrain]
      â†“
[Video Thumbnails/Frames] â€”â€”â†’ [BLIP-2 / CLIP / OFA]
      â†“
[Aggregate Features] â€”â€”â†’ [DataFrame / CSV]


The core `train.py` file runs a full PyCaret-based regression training pipeline:

- Loads and validates data from `data/raw/*.json`
- Cleans and filters data for modeling
- Applies multicollinearity reduction, variance filtering
- Trains a `LightGBM` model with `tune_model()` using Bayesian optimization
- Saves predictions, residuals, charts, and model artifacts in `charts/`

### ğŸ—‚ Output Files

- `charts/`: model diagnostic plots
- `charts/predictions.csv`: actual vs predicted
- `charts/metrics.csv`: performance metrics (RÂ², RMSE, MAE)
- `model.pkl`: final model

---

## ğŸ¤– LLM Benchmarking

The project also fine-tunes and evaluates lightweight LLMs on benchmark NLP tasks.

### ğŸ”§ Models Used

```yaml
- nlptown/bert-base-multilingual-uncased-sentiment (max_length: 512)
- deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B (max_length: 2048)
- meta-llama/Llama-3.2-1B (max_length: 2048)
```
